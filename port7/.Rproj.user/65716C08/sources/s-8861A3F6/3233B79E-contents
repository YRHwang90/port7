---
title: "portfolio7"
author: "Yoo Ri Hwang"
date: "3/9/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

In this portfolio, I aim to be familair with *basic* simulation 

## Source & Reference 

1) Introducing Monte Carlo Methods with R (Robert & Casella, 2010)
2) https://towardsdatascience.com/generate-simulated-dataset-for-linear-model-in-r-469a5e2f4c2e   
**3) https://datascience4psych.github.io/DataScience4Psych/welcome-to-simulating-data.html** 
4) https://cran.r-project.org/web/packages/holodeck/vignettes/simulating-data.html  
5) https://blog.revolutionanalytics.com/2016/02/multivariate_data_with_r.html  
6. https://s3.amazonaws.com/assets.datacamp.com/production/course_6521/slides/chapter2.pdf




## packages

```{r}
library(ggfortify)
# install.packages('ggfortify')

```

## Univariate Normal & the one-sample t-test 

```{r}


x<-rnorm(25, mean=0, sd=100)
# produce N(0.3, 20) sample of size 25. 
print(x)

t.test(x) # one sample t-test to see if the true mean score would be 0 or not 

#p-value: 0.99. 



```

Although I generate the x data to be mean of zero, Sd was unrealistically high. 


```{r}


y<-rnorm(25, mean=0, sd=1)
# produce N(0.3, 20) sample of size 25. 
print(y)

t.test(y) # one sample t-test to see if the true mean score would be 0 or not 

#p-value becomes much lower than t.test(x)

```

## Simple regression 

```{r}
set.seed(1)
x=seq(-3,3,le=5)
print(x)
y=2+4*x+rnorm(5)
lm(y~x)


```



$$Y_{ij}=\alpha + \beta*x_{i} + e_{ij}$$
$$\hat{\alpha} = 2.13$$
$$\hat{\beta} = 4.22$$


### character variable

```{R}

group=rep(c("female","male"), each=3)
response=rnorm (n=6, mean=0, sd=2)
data.frame(group,response)
```

### replicate()

```{r}

replicate(n = 3, 
          expr = rnorm(n = 5, mean = 0, sd = 1),
          simplify=FALSE) # list output 


replicate(n = 3, 
          expr = rnorm(n = 5, mean = 0, sd = 1),
          )

```

#### loop

```{r}

list1=list() # empty list to save output in. 
for(i in 1:3){
  list1[[i]]=rnorm(n=10,mean=0,sd=1)
}

print(list1)

```



### multiple regression


```{r}

set.seed(99)

x1=rnorm(100,50,9)

x2=rbinom(100,1,0.5)
# 
# x2<-ifelse(x2==0,"male","female")
# x2<-as.factor(x2)

error<-rnorm(100,0,10)

# generating DV

y=3*x1+2*x2+error

# creating model

model1<-lm(y~x1+x2)
summary(model1)
autoplot(model1)
```


##  Bivariate normal

```{r}

library(MASS)

biv<-mvrnorm(100, # sample size
            mu=c(0,0), # Mu
            Sigma = matrix(c(1,0.5,1,0.5),2)) # Covariance matrix

print(head(biv,10))
```

visualization

```{r}

# kernel density estimation 
# TMI: Yoo Ri currently did not learn what kernel density is. 

biv.kd<-kde2d(biv[,1], biv[,2], n=50)
image(biv.kd)
contour(biv.kd, add=TRUE)



```


## Multivariate Normal Simulation

 Generating multivariate normal data 
```{r}

library(MASS)

#creating random multivariate data that has
# sample size of 50, mean of 0.5,0,10, and covariance matrix is as sigma

mul1 <- MASS::mvrnorm(n = 50,       
                 mu = c(0.5, 0, 10),
                 Sigma = matrix(c(1, 0.2, 0.3,
                                  0.2, 1, 0.6,
                                  0.3, 0.6, 1),
                                nrow = 3))

print(mul1)

```



Correlation matrix
```{r}


cor(mul1)

```



```

```